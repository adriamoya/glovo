{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Evaluating the Model\n",
    "\n",
    "Choose an evaluation metric fitting to your model and explain why you have chosen this and share your scores for this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assessment we have considered AUC (Area Under the ROC Curve) since we are interested in ordering properly the output probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from file\n",
    "xgb = pickle.load(open(\"xgb.pickle.dat\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('./data/Courier_data_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'flag'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df[df['train']==1]\n",
    "test = df[df['train']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors = [x for x in df.columns if x not in ['courier', 'flag', 'feature_5', 'lifetime_1', 'train']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Report (Train)\n",
      "AUC (pred) : 0.9906\n",
      "AUC (prob) : 0.9996\n",
      "Accuracy   : 0.9919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/moyandreu/coding/glovo/Data_Scientist_Interview/env/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/Users/moyandreu/coding/glovo/Data_Scientist_Interview/env/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/moyandreu/coding/glovo/Data_Scientist_Interview/env/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Predict training set:\n",
    "train['pred'] = xgb.predict(train[predictors])\n",
    "train['pred_proba'] = xgb.predict_proba(train[predictors])[:,1]\n",
    "\n",
    "print(\"\\nModel Report (Train)\")\n",
    "print(\"AUC (pred) : %.4g\" % metrics.roc_auc_score(train[target].values, train['pred'].values))\n",
    "print(\"AUC (prob) : %.4g\" % metrics.roc_auc_score(train[target].values, train['pred_proba'].values))\n",
    "print(\"Accuracy   : %.4g\" % metrics.accuracy_score(train[target].values, train['pred'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict test set:\n",
    "test['pred'] = xgb4.predict(test[predictors])\n",
    "test['pred_proba'] = xgb4.predict_proba(test[predictors])[:,1]\n",
    "\n",
    "print(\"\\nModel Report (Test)\")\n",
    "print(\"AUC (pred) : %.4g\" % metrics.roc_auc_score(test[target].values, test['pred'].values))\n",
    "print(\"AUC (prob) : %.4g\" % metrics.roc_auc_score(test[target].values, test['pred_proba'].values))\n",
    "print(\"Accuracy   : %.4g\" % metrics.accuracy_score(test[target].values, test['pred'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2722    0.196644\n",
       "2723    0.037773\n",
       "2724    0.779489\n",
       "2725    0.591403\n",
       "2726    0.061644\n",
       "2727    0.703403\n",
       "2728    0.669469\n",
       "2729    0.631626\n",
       "2730    0.495539\n",
       "2731    0.970125\n",
       "2732    0.030414\n",
       "2733    0.758315\n",
       "2734    0.920025\n",
       "2735    0.021633\n",
       "2736    0.167261\n",
       "2737    0.487813\n",
       "2738    0.537097\n",
       "2739    0.453893\n",
       "2740    0.715237\n",
       "2741    0.694736\n",
       "2742    0.577902\n",
       "2743    0.464089\n",
       "2744    0.259008\n",
       "2745    0.040605\n",
       "2746    0.226068\n",
       "2747    0.068936\n",
       "2748    0.566523\n",
       "2749    0.081247\n",
       "2750    0.086212\n",
       "2751    0.634008\n",
       "          ...   \n",
       "2995    0.873572\n",
       "2996    0.007071\n",
       "2997    0.029374\n",
       "2998    0.098914\n",
       "2999    0.007624\n",
       "3000    0.911153\n",
       "3001    0.010180\n",
       "3002    0.039715\n",
       "3003    0.417509\n",
       "3004    0.163603\n",
       "3005    0.074626\n",
       "3006    0.815285\n",
       "3007    0.760939\n",
       "3008    0.056059\n",
       "3009    0.132372\n",
       "3010    0.026950\n",
       "3011    0.814407\n",
       "3012    0.031581\n",
       "3013    0.507917\n",
       "3014    0.535554\n",
       "3015    0.376162\n",
       "3016    0.211399\n",
       "3017    0.027493\n",
       "3018    0.034735\n",
       "3019    0.613844\n",
       "3020    0.906572\n",
       "3021    0.598027\n",
       "3022    0.121619\n",
       "3023    0.763408\n",
       "3024    0.059702\n",
       "Name: pred_proba, Length: 303, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['pred_proba']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = [round(value) for value in y_test_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 77.89%\n",
      "AUC: 76.6578%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "# evaluate predictions\n",
    "accuracy = accuracy_score(test['flag'].values, y_test_pred)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "print(\"AUC: %.4f%%\" % (100*roc_auc_score(test['flag'].values, y_test_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
